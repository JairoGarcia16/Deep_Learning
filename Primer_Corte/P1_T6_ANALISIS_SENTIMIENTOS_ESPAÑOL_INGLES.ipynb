{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P1_T6_ANALISIS_SENTIMIENTOS_ESPAÑOL_INGLES.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "g3w7xhrG0_q5",
        "54aCGRWaPPq6",
        "dtzJHYMJcQz3",
        "5E84yXpedZQ3",
        "5Rc9RsrNNbxI",
        "HZ4mzeX-NfBu",
        "IhCauzL5jkpN",
        "LrG7haSNtb6T",
        "xvec1PiF5Ipz",
        "j6wQAhv0V4RU",
        "zm6bJ8rDnJ8d",
        "YYuKjPlHnvdV",
        "h6a_3I3ZoTc6",
        "c0t1IZY6EzcB",
        "eqoOWZwOsD6w",
        "W712Ry66vFjl",
        "UwBHQQFsqR7i",
        "WlERt98W5qLZ",
        "IrJ7vHEWNAbd",
        "OTB4zzjoL1Rs",
        "HCCRddegNIDA",
        "p3qBGgvpRgZL",
        "FCS2HmN4V_hi",
        "-aWOdOb167P7",
        "OIJQSCntSPOl",
        "IOE6fCqXcUXn",
        "-3kl_mdMN4ec",
        "4fcBk57OXCz3",
        "MiZauNoCbrxG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Analisis de Datos Twitter\n",
        "#*@Autor:Jairo Eduardo Garcia Sisa\n",
        "#*@Date:19/03/2022\n",
        "#*@Description:\n",
        "###Exersice for twitter analysis"
      ],
      "metadata": {
        "id": "o-l5JO7o9VSd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDXCWuClqs4S"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#**ANALISIS DE SENTIMIENTOS EN ESPAÑOL USANDO TWITTER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0Ztco6-b09l"
      },
      "source": [
        "##Obtener Dataset de twitter para analizar\n",
        "https://github.com/jcsobrino/TFM-Analisis_sentimientos_Twitter-UOC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtzJHYMJcQz3"
      },
      "source": [
        "###cargar librerias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esiw0R5hb9Ob"
      },
      "source": [
        "import os\n",
        "import tweepy as tw\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E84yXpedZQ3"
      },
      "source": [
        "###permisos de acceso desde python a el api rest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_hwgEk2culC"
      },
      "source": [
        "consumer_key= 'XB1WKfL2MPTMVqDjAZRzzz594'\n",
        "consumer_secret= 'C49IjWS4a0zdBFPNhhSshrt9O4a78GTsq8mtSoasHRIeCFhVuf'\n",
        "access_token= '1387929113038557185-uTG7sdJq3jZoC7XM8xVbKeG5qYnhlp'\n",
        "access_token_secret= '3KpdZar3L7rT2HLXkbEfn7B8T5GcaQXlpnD4ebXARrE6a'\n",
        "\n",
        "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tw.API(auth)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Rc9RsrNNbxI"
      },
      "source": [
        "###Subiendo un tweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roIJgdUoden3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "e79f9511-1ccf-4144-c15c-fd6a95a43942"
      },
      "source": [
        "api.update_status('#USTATUNJA,#DEEP_LEARNING Generación de Tweet con libreria tweepy desde PYTHON')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TweepError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTweepError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-9fbd39105272>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'#USTATUNJA,#DEEP_LEARNING Generación de Tweet con libreria tweepy desde PYTHON'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/api.py\u001b[0m in \u001b[0;36mupdate_status\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m                            'card_uri'],\n\u001b[1;32m    217\u001b[0m             \u001b[0mrequire_auth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         )(*args, **kwargs)\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmedia_upload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/binder.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/binder.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mRateLimitError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mTweepError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_error_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;31m# Parse the response payload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTweepError\u001b[0m: Read-only application cannot POST."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al buscar una posible solución me aparece que debo dar mas permisos, no solo lectura, pero en la sección de developer twitter no veo para poder dar estas funciones. :("
      ],
      "metadata": {
        "id": "n_2xP4O7-OUS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ4mzeX-NfBu"
      },
      "source": [
        "###Consultando tweets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzjhHn6vNrMG"
      },
      "source": [
        "# Definir el termino de la busqueda y la fecha de inicio\n",
        "search_words ='Jamming'\n",
        "date_since ='2022-01-01'\n",
        "#para que no tome los tweets que estar retweets\n",
        "new_search = search_words+\" -filter:retweets\"\n",
        "new_search\n",
        "\u000b# Collecional tweets\n",
        "tweets = tw.Cursor(api.search,new_search,\"es\",date_since).items(1000)\n",
        "#[tweet.text for tweet in tweets] "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnsyptWENvg3"
      },
      "source": [
        "###Convertimos los tweets en una Dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reYRHI2zN0oG"
      },
      "source": [
        "data_frame = [[tweet.user.screen_name, tweet.user.location,tweet.text] for tweet in tweets]\n",
        "\n",
        "tw_dataframe = pd.DataFrame(data= data_frame , columns=[\"user\",\"location\",\"text\"])\n",
        "tw_dataframe\n",
        "#guardamos el dataframe en un CSV\n",
        "tw_dataframe.to_csv('/content/Jamming.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgKDHgt8IRkz"
      },
      "source": [
        "tw_dataframe.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxiaP7qb2HIk"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktPgWvbqOEBm"
      },
      "source": [
        "##P0. Obtener corpus : Convirtiendo XML a CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_N996vYOx2c"
      },
      "source": [
        "#librerias necesarias\n",
        "import xml.etree.ElementTree as etree\n",
        "import csv\n",
        "from os import scandir\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhCauzL5jkpN"
      },
      "source": [
        "###Funcion para listar archivos de un directorio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQfljH0ukvuM"
      },
      "source": [
        "# importing os module  \n",
        "import os \n",
        "#listado_de_archivos_desde_un_path\n",
        "def files_of_path(path): \n",
        "    return [obj.name for obj in os.scandir(path) if obj.is_file()]\n",
        "    \n",
        "files= files_of_path(\"/content/drive/MyDrive/IA/Analisis_sentimientos_Twitter/datasets/Corpus/tass_2017\")\n",
        "for file in files:\n",
        "    print(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrG7haSNtb6T"
      },
      "source": [
        "###función para convertir listas en archivos CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGxsb68nt1Co"
      },
      "source": [
        "def list_to_csv(data, filename):\n",
        "  with open(filename, 'w', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile, delimiter=',', lineterminator='\\n', quoting=csv.QUOTE_NONNUMERIC)\n",
        "    writer.writerows(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvec1PiF5Ipz"
      },
      "source": [
        "###función para cargar de un CSV a  una LISTA (messages | labels)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBAnptZ04-We"
      },
      "source": [
        "def csv_to_lists(filename):\n",
        "  messages = []\n",
        "  labels = []\n",
        "  with open(filename, 'r', encoding='utf-8') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=',')\n",
        "    for row in reader:\n",
        "      messages.append(row[1])\n",
        "      labels.append(row[2])\n",
        "  return messages, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6wQAhv0V4RU"
      },
      "source": [
        "###funciones para prasear xml en un dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm6bJ8rDnJ8d"
      },
      "source": [
        "####corpus de general  tweetid | content | sentiments/polarity/value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSZHjRsUV9wj"
      },
      "source": [
        "def general_tass_to_list(filename):\n",
        "  tree = etree.parse(filename)\n",
        "  root = tree.getroot()\n",
        "  data = []\n",
        "\n",
        "  for tweet in root:\n",
        "    tweetId = tweet.find('tweetid').text\n",
        "    content = tweet.find('content').text\n",
        "    polarityValue = tweet.find('sentiments/polarity/value').text\n",
        "    data.append([tweetId, content.replace('\\n',' '), polarityValue])\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkQa78F3K6Vp"
      },
      "source": [
        "def general_tass_2017_to_list(filename,qrel=None):\n",
        "  tree = etree.parse(filename)\n",
        "  root = tree.getroot()\n",
        "  data = []\n",
        "\n",
        "  for tweet in root:\n",
        "    tweetId = tweet.find('tweetid').text\n",
        "    content = tweet.find('content').text\n",
        "    polarityValue = qrel[tweetId]\n",
        "    data.append([tweetId, content.replace('\\n',' '), polarityValue])\n",
        "\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYuKjPlHnvdV"
      },
      "source": [
        "####Corpus politics  tweetid | content | sentiments/polarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iybrjrHmnvzs"
      },
      "source": [
        "def politics_tass_to_list(filename):\n",
        "  tree = etree.parse(filename)\n",
        "  root = tree.getroot()\n",
        "  data = []\n",
        "\n",
        "  for tweet in root:\n",
        "    tweetId = tweet.find('tweetid').text\n",
        "    content = tweet.find('content').text\n",
        "    aux = next((e for e in tweet.findall('sentiments/polarity') if e.find('entity') == None), None)\n",
        "    if aux != None:\n",
        "      polarityValue = aux.find('value').text\n",
        "      data.append([tweetId, content.replace('\\n',' '), polarityValue])\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6a_3I3ZoTc6"
      },
      "source": [
        "####corpus de internacional  tweetid | content | sentiments/polarity/value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGKq5CTgoT2h"
      },
      "source": [
        "def intertass_tass_to_list(filename, qrel=None):\n",
        "  tree = etree.parse(filename)\n",
        "  root = tree.getroot()\n",
        "  data = []\n",
        "\n",
        "  for tweet in root:\n",
        "    tweetId = tweet.find('tweetid').text\n",
        "    content = tweet.find('content').text\n",
        "    polarityValue = tweet.find('sentiment/polarity/value').text\n",
        "    if polarityValue == None:\n",
        "      polarityValue = qrel[tweetId]\n",
        "      data.append([tweetId, content.replace('\\n',' '), polarityValue])\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0t1IZY6EzcB"
      },
      "source": [
        "####Parcear corpus de social-TV (no esta terminado)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXVrTziIUB8x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b9e1f823-286e-407d-f1f9-a1604ad1d1eb"
      },
      "source": [
        "from lxml import etree\n",
        "doc = etree.parse('/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/datasets/tass_2017/Social-TV/socialtv-tweets-test.xml')\n",
        "#print (etree.tostring(doc,pretty_print=True ,xml_declaration=True, encoding=\"utf-8\"))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tweets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHREvJ4MUqQp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "760377aa-f7b1-4bd4-ddc9-c60288672949"
      },
      "source": [
        "raiz=doc.getroot()\n",
        "#print (raiz.tag)\n",
        "#print (len(raiz))\n",
        "tweet=raiz[1]\n",
        "print (tweet.text)\n",
        "print (tweet.attrib)\n",
        "print (tweet[0].text)\n",
        "for attr,value in tweet.items():\n",
        "  print (attr,value)\n",
        "print (tweet.get(\"sentiment\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El \n",
            "{'id': '456544890499760129'}\n",
            "Barça\n",
            "id 456544890499760129\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6cEJOPYP92J"
      },
      "source": [
        "from xml.dom import minidom\n",
        "def social_tv_to_list(filename):\n",
        "  xmldoc = minidom.parse(filename)\n",
        "  tweetlist = xmldoc.getElementsByTagName('tweet')\n",
        "  print(len(tweetlist))\n",
        "  print(tweetlist[0].attributes['id'].value)\n",
        "  sentimentlist = xmldoc.getElementsByTagName('sentiment')\n",
        "  print(sentimentlist[0].attributes['aspect'].value)\n",
        "  #for s in tweetlist:\n",
        "#    print(s.attributes['id'].value)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulBbULqvQg10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1b8ddb7a-bdd6-4901-90ca-b62fb6bfc729"
      },
      "source": [
        "social_tv_to_list(\"/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/datasets/tass_2017/Social-TV/socialtv-tweets-test.xml\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "456544890097131521\n",
            "Entrenador\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqoOWZwOsD6w"
      },
      "source": [
        "####funcion para unir los tweets corpus general test con sus sentimientos\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YY91YATsEOq"
      },
      "source": [
        "#Listar los id tweets | sentiment :P (Positivo) - N (Negativo) - NEU (NEUtro) - NONE (sin sentimiento)\n",
        "def gold_standard_to_dict(filename):\n",
        "  with open(filename, 'r') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter='\\t')\n",
        "    data = {rows[0]: rows[1] for rows in reader}\n",
        "\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W712Ry66vFjl"
      },
      "source": [
        "###Función para separar el 100% del corpus entre: Train : 70% - Test: 30%\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcDxtXKbvN4V"
      },
      "source": [
        "def generate_train_test_subsets(data, size):\n",
        "  codes = [d[0] for d in data]\n",
        "  labels = [d[2] for d in data]\n",
        "  codes_train, codes_test, labels_train, labels_test = train_test_split(codes, labels, train_size=size)\n",
        "  train_data = [d for d in data if d[0] in codes_train]\n",
        "  test_data = [d for d in data if d[0] in codes_test]\n",
        "  return train_data, test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwBHQQFsqR7i"
      },
      "source": [
        "### **Ejecutar cada función de parsear los corpus y guardarlo en un CSV (full, train, test)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QImbKpQOgVu1"
      },
      "source": [
        "!git clone 'https://github.com/luisFernandoCastellanosG/Machine_learning'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FmpPZyXqcyX"
      },
      "source": [
        "data = []\n",
        "\n",
        "#Parceamos el internacional TASS\n",
        "#tomamos el corpus internacional (test) y generamos una lista del ID del tweet y el sentimiento para agregarlo a la data\n",
        "qrel = gold_standard_to_dict(\"/content/drive/MyDrive/IA/Analisis_sentimientos_Twitter/datasets/Corpus/tass_2017/InterTASS/InterTASS_Test_res.qrel\")\n",
        "#como el test del corpus internacional esta sin los sentimientos es necesario agregarlos : qrel\n",
        "data.extend(intertass_tass_to_list(\"/content/drive/MyDrive/IA/Analisis_sentimientos_Twitter/datasets/Corpus/tass_2017/InterTASS/InterTASS_Test.xml\", qrel))\n",
        "data.extend(intertass_tass_to_list(\"/content/drive/MyDrive/IA/Analisis_sentimientos_Twitter/datasets/Corpus/tass_2017/InterTASS/InterTASS_development.xml\"))\n",
        "data.extend(intertass_tass_to_list(\"/content/drive/MyDrive/IA/Analisis_sentimientos_Twitter/datasets/Corpus/tass_2017/InterTASS/InterTASS_Training.xml\"))\n",
        "#Parceamos el General\n",
        "#NO-data.extend(DatasetHelper.general_tass_to_list(\"../datasets/tass_2017/InterTASS/general-test-tagged-3l.xml\"))\n",
        "#NO-data.extend(DatasetHelper.general_tass_to_list(\"../datasets/tass_2017/InterTASS/general-train-tagged-3l.xml\"))\n",
        "qrel = gold_standard_to_dict(\"/content/drive/MyDrive/IA/Analisis_sentimientos_Twitter/datasets/Corpus/tass_2017/General Corpus of TASS/general-sentiment-3l.qrel\")\n",
        "data.extend(general_tass_2017_to_list(\"/content/drive/MyDrive/IA/Analisis_sentimientos_Twitter/datasets/Corpus/tass_2017/General Corpus of TASS/general-tweets-test.xml\", qrel))\n",
        "#Parceamos el STOMPOL (politica)\n",
        "data.extend(politics_tass_to_list(\"/content/drive/MyDrive/IA/Analisis_sentimientos_Twitter/datasets/Corpus/tass_2014/politics-test-tagged.xml\"))\n",
        "#separamos la data en train = 70%  | test = 30#\n",
        "test, train  = generate_train_test_subsets(data, size=0.3)\n",
        "list_to_csv(data,\"/content/dataset_2017_full.csv\")\n",
        "list_to_csv(train, '/content/dataset_2017_train.csv')\n",
        "list_to_csv(test, '/content/dataset_2017_test.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRF-yQ5j4qR6"
      },
      "source": [
        "##P1. Preprocesamiento del corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlERt98W5qLZ"
      },
      "source": [
        "###Cargar librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ0W5EYT5s2B"
      },
      "source": [
        "import re                                #operaciones regulares para la búsqueda y manipulación de cadenas\n",
        "from nltk import TweetTokenizer          #libreria para tokenizar\n",
        "from nltk.stem import SnowballStemmer    #algoritmo para clasificación de palabras\n",
        "#variables para mejorar la escritura (opcional)\n",
        "NORMALIZE = 'normalize'\n",
        "REMOVE = 'remove'\n",
        "MENTION = 'twmention'\n",
        "HASHTAG = 'twhashtag'\n",
        "URL = 'twurl'\n",
        "LAUGH = 'twlaugh'\n",
        "\n",
        "#definir que el algoritmo de clasificación use el idioma español\n",
        "_stemmer = SnowballStemmer('spanish')\n",
        "\n",
        "#definir una variable para la funcion de tokenizar (opcional)\n",
        "_tokenizer = TweetTokenizer().tokenize\n",
        "\n",
        "#variable para definir si quiero normalizar: normalize o eliminar: remove los hashtags, menciones y urls en los tweets\n",
        "_twitter_features=\"normalize\"\n",
        "#variable para definir si se desea tener convertir o no a la raiz de la palabra.\n",
        "_stemming=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrJ7vHEWNAbd"
      },
      "source": [
        "### funciones/métodos de preprocesamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTB4zzjoL1Rs"
      },
      "source": [
        "####listas de conversión (quitar tildes y palabras coloquiales) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Tj86lDGL6cU"
      },
      "source": [
        "#lista de conversión para quitar las tildes a las vocales.\n",
        "DIACRITICAL_VOWELS = [('á','a'), ('é','e'), ('í','i'), ('ó','o'), ('ú','u'), ('ü','u')]\n",
        "\n",
        "#lista para corregir algunas palabras coloquiales / jerga en español (obviamente faltan más)\n",
        "SLANG = [('d','de'), ('[qk]','que'), ('xo','pero'), ('xa', 'para'), ('[xp]q','porque'),('es[qk]', 'es que'),\n",
        "         ('fvr','favor'),('(xfa|xf|pf|plis|pls|porfa)', 'por favor'), ('dnd','donde'), ('tb', 'también'),\n",
        "         ('(tq|tk)', 'te quiero'), ('(tqm|tkm)', 'te quiero mucho'), ('x','por'), ('\\+','mas')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCCRddegNIDA"
      },
      "source": [
        "#### funcion/método de normalización de risas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lvGhh5vNMEc"
      },
      "source": [
        "#metodo para normalizar las risas\n",
        "def normalize_laughs(message):\n",
        "  message = re.sub(r'\\b(?=\\w*[j])[aeiouj]{4,}\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'\\b(?=\\w*[k])[aeiouk]{4,}\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'\\b(?=\\w*[h])[aeiouk]{4,}\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'\\b(juas+|lol)\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  return message"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOW2UQjqN6Eh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dc55a1a-800e-47c8-baef-c4fc05ab2d32"
      },
      "source": [
        "print (normalize_laughs(\"esto muyy feliz jajajajaja o no tan feliz jejejejeje o mejor me rio a como papa noel JOJOJO o como en mileniams LOL  kakaka\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "esto muyy feliz twlaugh o no tan feliz twlaugh o mejor me rio a como papa noel twlaugh o como en mileniams twlaugh  twlaugh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3qBGgvpRgZL"
      },
      "source": [
        "####Función/método para eliminar o normalizar  menciones, hashtags y URL de un mensaje (tweet)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpdPu3K8RukN"
      },
      "source": [
        "def process_twitter_features(message, twitter_features):\n",
        "\n",
        "  message = re.sub(r'[\\.\\,]http','. http', message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'[\\.\\,]#', '. #', message)\n",
        "  message = re.sub(r'[\\.\\,]@', '. @', message)\n",
        "\n",
        "  if twitter_features == REMOVE:\n",
        "    # eliminar menciones, hashtags y URL\n",
        "    message = re.sub(r'((?<=\\s)|(?<=\\A))(@|#)\\S+', '', message)\n",
        "    message = re.sub(r'\\b(https?:\\S+)\\b', '', message, flags=re.IGNORECASE)\n",
        "  elif twitter_features == NORMALIZE:\n",
        "    # cuando sea necesario se normalizaran las menciones, hashtags y URL\n",
        "    message = re.sub(r'((?<=\\s)|(?<=\\A))@\\S+', MENTION, message)\n",
        "    message = re.sub(r'((?<=\\s)|(?<=\\A))#\\S+', HASHTAG, message)\n",
        "    message = re.sub(r'\\b(https?:\\S+)\\b', URL, message, flags=re.IGNORECASE)\n",
        "\n",
        "  return message"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS3BzP5OR_hJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c9c31fb-b93a-4ffb-91f3-b07a0f63cf1a"
      },
      "source": [
        "print(process_twitter_features(\"Rosell, una noche. Adivina quien!! http://t.co/PPAwijRX, jajajAja dime si NO ES DÍVERTIDÓÓ\",\"normalize\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rosell, una noche. Adivina quien!! twurl, jajajAja dime si NO ES DÍVERTIDÓÓ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCS2HmN4V_hi"
      },
      "source": [
        "####Función/método general para el preprocesamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3T1DgbnWFYG"
      },
      "source": [
        "def preprocess(message):\n",
        "  # convertir a minusculas\n",
        "  message = message.lower()\n",
        "        \n",
        "  # eliminar números, retorno de linea y el tan odios retweet (de los viejos estilos de twitter)\n",
        "  message = re.sub(r'(\\d+|\\n|\\brt\\b)', '', message)\n",
        "        \n",
        "  # elimar vocales con signos diacríticos (posible ambigüedad)\n",
        "  for s,t in DIACRITICAL_VOWELS:\n",
        "    message = re.sub(r'{0}'.format(s), t, message)\n",
        "        \n",
        "  # eliminar caracteres repetidos \n",
        "  message = re.sub(r'(.)\\1{2,}', r'\\1\\1', message)\n",
        "       \n",
        "  # normalizar las risas\n",
        "  message = normalize_laughs(message)\n",
        "        \n",
        "  # traducir la jerga y terminos coloquiales sobre todo en el español\n",
        "  for s,t in SLANG:\n",
        "    message = re.sub(r'\\b{0}\\b'.format(s), t, message)\n",
        "\n",
        "  #normalizar/eliminar hashtags, menciones y URL\n",
        "  message = process_twitter_features(message, _twitter_features)\n",
        "\n",
        "  #Convertir las palabras a su raiz ( Bonita, bonito) -> bonit \n",
        "  if _stemming:\n",
        "    message = ' '.join(_stemmer.stem(w) for w in _tokenizer(message))\n",
        "\n",
        "  return message"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCkSi7RJW6FX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8affb2f2-e65b-4334-e2e6-583ea9f6c79d"
      },
      "source": [
        "print(preprocess(\"LOL!! muy graciosa esta paguina https://actualidadpanamericana.com :-) jajajaja muy buena\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "twlaugh!! muy graciosa esta paguina twurl :-) twlaugh muy buena\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aWOdOb167P7"
      },
      "source": [
        "###Descargamos la librerias  NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSp_L2h2_I36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41b51e69-80d8-4ee8-c53a-d31579fd190c"
      },
      "source": [
        "#Descargamos la libreria de stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIJQSCntSPOl"
      },
      "source": [
        "### Cargamos el CSV del corpus de google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVtJMBDy_slb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35e16d27-9f76-4c55-8d8c-cc4e65541441"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOE6fCqXcUXn"
      },
      "source": [
        "###Aplicamos preprocesamiento al CSV y creamos un nuevo CSV limpio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ods0bwtGCWo"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/luisFernandoCastellanosG/Machine_learning/master/Analisis_sentimientos_Twitter/espanish/datasets/Corpus/dataset_2017_full.csv', encoding='utf-8')\n",
        "#asignamos nombres a las columnas del csv para facilitar la busqueda de información\n",
        "df.columns = ['tweetid', 'tweet','sentiment']\n",
        "#aplicamos el preprocesamiento a los tweets con steaming =false\n",
        "df['tweet'] = df['tweet'].apply(preprocess)\n",
        "#eliminamos la columna tweetid que no nos sirve para entrenar y si nos genera mas uso de memoria \n",
        "df = df.drop(columns=\"tweetid\")\n",
        "#Es mejor trabajar con valores enteros que con letras\n",
        "#por lo tanto reemplazaremos los sentimientos que estan como NONE->-1 | NEU -> 0 | P->1 | N->2\n",
        "df.loc[df['sentiment'] == 'NONE', 'sentiment'] = '-1'\n",
        "df.loc[df['sentiment'] == 'NEU', 'sentiment'] = '0'\n",
        "df.loc[df['sentiment'] == 'P', 'sentiment'] = '1'\n",
        "df.loc[df['sentiment'] == 'N', 'sentiment'] = '2'\n",
        "df[\"sentiment\"].unique()\n",
        "#guardamos el dataset en un nuvevo CSV para facilitar su posterior uso\n",
        "df.to_csv('/content/dataset_2017_full_clean.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAn4p8Z_Y8sH"
      },
      "source": [
        "##P2.Entrenando el modelo de aprendizaje"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3kl_mdMN4ec"
      },
      "source": [
        "###Funciones de tokenizar/extraer tweets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5UTVDCZZBYs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ee46126-2fd3-4cb4-9326-209c0651f43b"
      },
      "source": [
        "#p2.1: funcion tokenizar con esteroides --tokeniza y limpia--\n",
        "print(\"p2.1: funcion tokenizar con esteroides --tokeniza y limpia--\")\n",
        "def tokenizer(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
        "    text = re.sub('[\\W]+', ' ', text.lower()) +' '.join(emoticons).replace('-', '')\n",
        "    tokenized = [w for w in text.split() if w not in stop]\n",
        "    return tokenized\n",
        "#p2.2: funcion para extraer un documento del dataset  \n",
        "print(\"p2.2: funcion para extraer un documento del dataset  \")\n",
        "def stream_docs(path):\n",
        "    with open(path, 'r', encoding='utf-8') as csv:\n",
        "        next(csv)  # skip header\n",
        "        for line in csv:\n",
        "            text, label = line[:-3],  int(line[-2])\n",
        "            yield text, label\n",
        "#p2.3: funcion que tomara una secuencia de documentos y devolvera un número particular de documentos\n",
        "def get_minibatch(doc_stream, size):\n",
        "    docs, y = [], []\n",
        "    try:\n",
        "        for _ in range(size):\n",
        "            text, label = next(doc_stream)\n",
        "            docs.append(text)\n",
        "            y.append(label)\n",
        "    except StopIteration:\n",
        "        return None, None\n",
        "    return docs, y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "p2.1: funcion tokenizar con esteroides --tokeniza y limpia--\n",
            "p2.2: funcion para extraer un documento del dataset  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuILfqd2hq1O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5dc7bf3-7d6e-4e33-c751-ed9e9f2def8a"
      },
      "source": [
        "next(stream_docs(path='/content/dataset_2017_full_clean.csv'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('twmention ya era hora de volver al csgo y dejares el padel bienvenida ', 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fcBk57OXCz3"
      },
      "source": [
        "###Entrenamos el modelo usando regresión logistica\n",
        "usaremos regresión logistica xq es menos costoso en tiempo de procesamiento que support vector machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCwTY8TCM3HE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e22b1870-d78f-43aa-da32-499c5d472758"
      },
      "source": [
        "path='/content/dataset_2017_full_clean.csv'\n",
        "#p2: definimos una versión liviana de CountVectorizer+TfidfVectorizer llamada HashingVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "vect = HashingVectorizer(decode_error='ignore', \n",
        "                         n_features=2**21,\n",
        "                         preprocessor=None, \n",
        "                         tokenizer=tokenizer)\n",
        "\n",
        "#definimos como algoritmo la regressión logistica en el decenso gradiante \n",
        "\n",
        "clf = SGDClassifier(loss='log', random_state=1, max_iter=1)\n",
        "doc_stream = stream_docs(path)\n",
        "#p3. entrenamos \n",
        "import re\n",
        "import numpy as np\n",
        "#import pyprind\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('spanish')\n",
        "#pbar = pyprind.ProgBar(50)\n",
        "#definimos las clases con las cuales vamos a entrenar\n",
        "classes = np.array([-1,0, 1,2])\n",
        "#hacemos 50 repeticiones\n",
        "for _ in range(50):\n",
        "  #tomaremos grupos de 500 tweets para entrenar\n",
        "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
        "    if not X_train:\n",
        "        break\n",
        "    X_train = vect.transform(X_train)\n",
        "    clf.partial_fit(X_train, y_train, classes=classes)\n",
        "    #pbar.update()\n",
        "#probamos la eficiencia del modelo con 5000 tweets .\n",
        "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
        "X_test = vect.transform(X_test)\n",
        "print('Presición del modelo: %.3f' % clf.score(X_test, y_test))\n",
        "#recalibramos el modelo.\n",
        "clf = clf.partial_fit(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Presición del modelo: 0.806\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLOtd2p7X0yn"
      },
      "source": [
        "##P3.Serializamos (congelamos) el modelo para usarlo fuera de google colaboratory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRxlktL3X-Q_"
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "#creo una carpeta en mi google drive para guardar los archivos serializados\n",
        "dest = os.path.join('/content/twitterclassifier', 'pkl_objects')\n",
        "if not os.path.exists(dest):\n",
        "    os.makedirs(dest)\n",
        "#convertimos el clasificador y el stopword en archivo/objectos pkl\n",
        "pickle.dump(stop, open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol=4)   \n",
        "pickle.dump(clf, open(os.path.join(dest, 'classifier.pkl'), 'wb'), protocol=4)\n",
        "#Es importante recordar que deben verificar que los dos archivos esten en su drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atFbFUcmZuwZ"
      },
      "source": [
        "###Probemos a ver si funciona\n",
        "Cambiamos la basepath (directorio por defecto) de Python a la carpeta de **Twitterclassifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moUaKeQOZye5"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/twitterclassifier') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWSBw2L6aJri"
      },
      "source": [
        "####Deserializamos los estimadores "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCQ2qYrsZ_67"
      },
      "source": [
        "import pickle\n",
        "import re\n",
        "import os\n",
        "from vectorizer import vect  # archivo vectorizer.py \n",
        "clf = pickle.load(open(os.path.join('/content/twitterclassifier/pkl_objects', 'classifier.pkl'), 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR3R_vU3aY_L"
      },
      "source": [
        "#### Clasifiquemos un texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVpOVv8tacSf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "079a8885-45c0-4c06-f4b3-0fcf5c071cf7"
      },
      "source": [
        "import numpy as np\n",
        "#NONE->-1 | NEU -> 0 | P->1 | N->2\n",
        "label = {-1:'Sin sentimiento', 0:'Neutro', 1:'Positivo',2: 'Negativo'}\n",
        "\n",
        "#example = ['Te odio más que a la muerte']\n",
        "example1 = 'amo este planeta, quiero ser feliz'\n",
        "example = [example1]\n",
        "#convertimos el texto en un vector de palabras y extraemos sus caracteristicas https://scikit-learn.org/stable/modules/feature_extraction.html\n",
        "textConvert = vect.transform(example)  \n",
        "print('*Predicción: %s\\n*Probabilidad: %.2f%%'%(label[clf.predict(textConvert)[0]], np.max(clf.predict_proba(textConvert))*100))\n",
        "print('*Predicción: %s'%label[clf.predict(textConvert)[0]])\n",
        "print(np.max(clf.predict_proba(textConvert))*100)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*Predicción: Positivo\n",
            "*Probabilidad: 87.70%\n",
            "*Predicción: Positivo\n",
            "87.70442129985135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiZauNoCbrxG"
      },
      "source": [
        "###**RECORREMOS LOS TWEETS DESCARGADOS Y LOS CLASIFICAMOS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL1EUqECSjP3"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyprind\n",
        "\n",
        "pbar = pyprind.ProgBar(50000)\n",
        "\n",
        "df = pd.read_csv('/content/EleccionesUSA2020_data.csv', encoding='utf-8')\n",
        "#creamos una columna llamada Sentimient donde guardaremos la predicción\n",
        "df['sentiment'] =''\n",
        "#creamos una columna llamada Probability donde guardaremos la acertabilidad que dio el clasificador\n",
        "df['probability']=0\n",
        "#conversión de sentimientos (numeros a palabras)= NONE->-1 | NEU -> 0 | P->1 | N->2\n",
        "label = {-1:'Sin sentimiento', 0:'Neutro', 1:'Positivo',2: 'Negativo'}\n",
        "for rowid in range(len(df.index)):\n",
        "  text=df['text'][rowid]\n",
        "  textConvert = vect.transform([text]) \n",
        "  df['sentiment'][rowid]=label[clf.predict(textConvert)[0]]\n",
        "  df['probability'][rowid]=np.max(clf.predict_proba(textConvert))*100\n",
        "  pbar.update()\n",
        "df.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZesOYw9FVF8e"
      },
      "source": [
        "\n",
        "df.to_csv('/content/EleccionesUSA2020_data_sentiment.csv'', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Noxdqzav-LF"
      },
      "source": [
        "#segunda forma de ejecutar el analisis (metodos)\n",
        "def f_prediction(row):\n",
        "  text=row['text']\n",
        "  textConvert = vect.transform([text]) \n",
        "  return label[clf.predict(textConvert)[0]]\n",
        "\n",
        "def f_probability(row):\n",
        "  text=row['text']\n",
        "  textConvert = vect.transform([text]) \n",
        "  return np.max(clf.predict_proba(textConvert))*100\n",
        "\n",
        "df[\"sentiment\"] = df.apply(f_prediction, axis=1) # recorriendo columnas\n",
        "df[\"probability\"] = df.apply(f_probability, axis=1) # recorriendo columnas\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Lh2gY17uFeG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "182b420d-952e-4c34-e879-b37e117c9335"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#sentimientos = df[\"sentiment\"].unique()\n",
        "df.groupby('sentiment')['location'].nunique().plot(kind='bar')\n",
        "print(df.groupby(['sentiment']).size())\n",
        "#df.groupby(['sentiment']).size().unstack().plot(kind='bar',stacked=True)\n",
        "plt.show()\n",
        "\n",
        "#df.head(20)\n",
        "#df[\"sentiment\"].unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentiment\n",
            "Negativo     64\n",
            "Positivo    936\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0KL67R0OhrU"
      },
      "source": [
        "##p4. generar DB de sqlite"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdCkpWREOl5Q"
      },
      "source": [
        "import sqlite3\n",
        "import os\n",
        "\n",
        "if os.path.exists('tweets.sqlite'):\n",
        "    os.remove('tweets.sqlite')\n",
        "\n",
        "conn = sqlite3.connect('tweets.sqlite')\n",
        "c = conn.cursor()\n",
        "c.execute('CREATE TABLE tweets_db (tweet TEXT, sentiment INTEGER, date TEXT)')\n",
        "\n",
        "example1 = 'que aburrido es estar en cuarentena…'\n",
        "c.execute(\"INSERT INTO tweets_db (tweet, sentiment, date) VALUES (?, ?, DATETIME('now'))\", (example1, 2))\n",
        "\n",
        "example2 = 'Estoy feliz de estar con mi familia'\n",
        "c.execute(\"INSERT INTO tweets_db (tweet, sentiment, date) VALUES (?, ?, DATETIME('now'))\", (example2, 1))\n",
        "\n",
        "conn.commit()\n",
        "conn.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGveeTAiO5SQ"
      },
      "source": [
        "###P4.1 consultar db"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qBTUzNEO8Xy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca19dd32-811a-490b-8354-3d97a9378a39"
      },
      "source": [
        "conn = sqlite3.connect('tweets.sqlite')\n",
        "c = conn.cursor()\n",
        "\n",
        "c.execute(\"SELECT * FROM tweets_db WHERE date BETWEEN '2019-01-01 10:10:10' AND DATETIME('now')\")\n",
        "results = c.fetchall()\n",
        "conn.close()\n",
        "\n",
        "print(results)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('que aburrido es estar en cuarentena…', 2, '2020-11-11 21:33:58'), ('Estoy feliz de estar con mi familia', 1, '2020-11-11 21:33:58')]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}